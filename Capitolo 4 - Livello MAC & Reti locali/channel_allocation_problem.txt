The central theme of this chapter is how to allocate a SINGLE broadcast channel among competing users. The channel might be a
portion of the wireless spectrum in a geographic region, or a single wire or optical fiber to which multiple nodes are
connected. It does not matter. In both cases, the channel connects each user to all other users and any user who makes full use
of the channel interferes with other users who also wish to use the channel.

--- STATIC CHANNEL ALLOCATION
The conventional way of allocating a single channel, such as a telephone trunk, among multiple competing users is to chop up
its capacity by using one of the multiplexing schemes we described in Sec. 2.4.4, such as FDM (Frequency Division Multiplexing).
If there are N users, the bandwidth is divided into N equal-sized portions, with each user being assigned one portion. Since each
user has a private frequency band, there is now no interference among users. When there is only a small and constant number of
users, each of which has a steady stream or a heavy load of traffic, this division is a simple and efficient allocation mechanism.
A wireless example is FM radio stations. Each station gets a portion of the FM band and uses it most of the time to broadcast its
signal.

However, when the number of senders is large and varying or the traffic is bursty, FDM presents some problems. If the spectrum is
cut up into N regions and fewer than N users are currently interested in communicating, a large piece of valuable spectrum will be
wasted. And if more than N users want to communicate, some of them will be denied permission for lack of bandwidth, even if some of
the users who have been assigned a frequency band hardly ever transmit or receive anything. Even assuming that the number of users
could somehow be held constant at N, dividing the single available channel into some number of static subchannels is inherently 
inefficient. The basic problem is that when some users are quiescent, their bandwidth is simply lost. They are not using it, and no
one else is allowed to use it either. A static allocation is a poor fit to most computer systems, in which data traffic is extremely
bursty, often with peak traffic to mean traffic ratios of 1000:1. Consequently, most of the channels will be idle most of the time.

--- ASSUMPTIONS FOR DYNAMIC CHANNEL ALLOCATION
1. Independent Traffic
    The model consists of N independent stations (e.g., computers, telephones), each with a program or user that generates frames
    for transmission. The expected number of frames generated in an interval of length delta_t is lambda*delta_t, where lambda is
    a constant (the arrival rate of new frames). Once a frame has been generated, the station is BLOCKED and does nothing until
    the frame has been successfully transmitted.
2. Single Channel
    A single channel is available for all communication. All stations can transmit on it and all can receive from it. The stations
    are assumed to be equally capable, though protocols may assign them different roles (e.g., priorities).
3. Observable Collisions
    If two frames are transmitted simultaneously, they overlap in time and the resulting signal is garbled. This event is called a
    collision. All stations can detect that a collision has occurred. A collided frame must be transmitted again later. No errors
    other than those generated by collisions occur.
4. Continuous or Slotted Time
    Time may be assumed continuous, in which case frame transmission can begin at any instant. Alternatively, time may be slotted
    or divided into discrete intervals (called slots). Frame transmissions must then begin at the start of a slot. A slot may
    contain 0, 1, or more frames, corresponding to an idle slot, a successful transmission, or a collision, respectively.
5. Carrier Sense or No Carrier Sense
    With the carrier sense assumption, stations can tell if the channel is in use before trying to use it. No station will attempt
    to use the channel while it is sensed as busy. If there is no carrier sense, stations cannot sense the channel before trying to
    use it. They just go ahead and transmit. Only later can they determine whether the transmission was successful.

Some discussion of these assumptions is in order. The first one says that frame arrivals are independent, both across stations and
at a particular station, and that frames are generated unpredictably but at a constant rate. Actually, this assumption is not a
particularly good model of network traffic, as it has long been well known that packets come in bursts over a range of time scales.

The single-channel assumption is the heart of the model. No external ways to communicate exist. Stations cannot raise their hands to
request that the teacher call on them, so we will have to come up with better solutions.

The remaining three assumptions depend on the engineering of the system, and we will say which assumptions hold when we examine a
particular protocol.

The collision assumption is basic. Stations need some way to detect collisions if they are to retransmit frames rather than let them
be lost. For wired channels, node hardware can be designed to detect collisions when they occur. The stations can then terminate
their transmissions prematurely to avoid wasting capacity. This detection is much harder for WIRELESS channels, so collisions are
usually INFERRED after the fact by the lack of an expected acknowledgement frame. 

The reason for the two alternative assumptions about time is that slotted time can be used to improve performance. However, it
requires the stations to follow a master clock or synchronize their actions with each other to divide time into discrete intervals.
Hence, it is not always available.

Similarly, a network may have carrier sensing or not. Wired networks will generally have carrier sense. WIRELESS networks cannot
always use it effectively because not every station may be within RADIO RANGE of every other station. Similarly, carrier sense
will not be available in other settings in which a station cannot communicate directly with other stations, for example a cable
modem in which stations must communicate via the cable headend. Note that the word ‘‘carrier’’ in this sense refers to a SIGNAL ON
THE CHANNEL and has nothing to do with the common carriers (e.g., telephone companies) that date back to the days of the Pony
Express.

NB: To avoid any misunderstanding, it is worth noting that NO multiaccess protocol guarantees RELIABLE delivery. Even in the
absence of collisions, the receiver may have copied some of the frame incorrectly for various reasons. Other parts of the link layer
or higher layers provide reliability.

--- MULTIPLE ACCESS PROTOCOLS
1) Pure ALOHA
The basic idea of an ALOHA system is simple: let users transmit whenever they have data to be sent. There will be collisions,
of course, and the colliding frames will be damaged. Senders need some way to find out if this is the case. In the ALOHA system,
after each station has sent its frame to the central computer, this computer REBROADCASTS the frame to all of the stations.
A sending station can thus listen for the broadcast from the hub to see if its frame has gotten through. (In other systems, such
as wired LANs, the sender might be able to listen for collisions while transmitting). If the frame was destroyed, the sender just
waits a random amount of time and sends it again. The waiting time MUST BE RANDOM or the same frames will collide over and over, in
lockstep. Systems in which multiple users share a common channel in a way that can lead to conflicts are known as contention systems.
    -> An interesting question is: what is the efficiency of an ALOHA channel? In other words, what fraction of all transmitted
    frames escape collisions under these chaotic circumstances? Il massimo che possiamo aspettarci con Pure ALOHA è che solo il 18%
    del tempo sul canale venga utilizzato per trasmettere dati corretti, senza collisioni. Il restante 82% del tempo è sprecato a
    causa delle collisioni o dei tempi di inattività. Questa bassa efficenza è causata dalla casualità dell'accesso.
2) Slotted ALOHA
Soon after ALOHA came onto the scene, Roberts (1972) published a method for doubling the capacity of an ALOHA system. His proposal
was to divide time into discrete intervals called slots, each interval corresponding to one frame. This approach requires the
users to agree on slot boundaries. One way to achieve synchronization would be to have one special station emit a pip at the start
of each interval, like a clock. In Roberts’ method, which has come to be known as slotted ALOHA — in contrast to Abramson’s pure
ALOHA — a station is not permitted to send whenever the user types a line. Instead, it is required to wait for the beginning of the
next slot. Thus, the continuous time ALOHA is turned into a discrete time one. This halves the vulnerable period, and doubles the 
capacity at roughly 37% 

--- CARRIER SENSE MULTIPLE ACCESS PROTOCOLS
With slotted ALOHA, the best channel utilization that can be achieved is 1/e. This low result is hardly surprising, since with
stations transmitting at will, without knowing what the other stations are doing there are bound to be many collisions. In LANs,
however, it is often possible for stations to detect what other stations are doing, and thus adapt their behavior accordingly.
These networks can achieve a much better utilization than 1/e. In this section, we will discuss some protocols for improving 
performance. Protocols in which stations listen for a carrier (i.e., a transmission) and act accordingly are called
Carrier Sense protocols.

3) Persistent CSMA
The first carrier sense protocol that we will study here is called 1-persistent CSMA (Carrier Sense Multiple Access). When a station
has data to send, it first listens to the channel to see if anyone else is transmitting at that moment. If the channel is idle,
the stations sends its data. Otherwise, if the channel is busy, the station just waits until it becomes idle. Then, the station
transmits a frame. If a collision occurs, the station waits a random amount of time and starts all over again. The protocol is
called 1-persistent because the station transmits with a probability of 1 (100%) when it finds the channel idle (in generale esistono
anche protocolli 'p'-persistent, cioè che trasmettono con probabilità p se trovano il canale libero, in questo modo le collisioni
diminuiscono ma la latenza aumenta).  

You might expect that this scheme avoids collisions except for the rare case of simultaneous sends, but in fact it does not. It’s
much worse than that. If two stations become ready in the middle of a third station’s transmission, both will wait politely until
the transmission ends, and then both will begin transmitting exactly simultaneously, resulting in a collision. If they were not so
impatient, there would be fewer collisions. More subtly, the PROPAGATION DELAY has a very important effect on collisions. There
is a chance that just after a station begins sending, another station will become ready to send and sense the channel. If the first
station’s signal has not yet reached the second one, the latter will sense an idle channel and will also begin sending, resulting
in a collision.

Even so, this protocol has better performance than pure ALOHA because both stations have the decency to desist from interfering with
the third station’s frame, so it gets through undamaged.

4) Non-persistent CSMA
A second carrier sense protocol is nonpersistent CSMA. In this protocol, a conscious attempt is made to be less greedy than in the
previous one. As before, a station senses the channel when it wants to send a frame, and if no one else is sending, the station
begins doing so itself immediately. However, if the channel is already in use, the station does not continually sense it for the 
purpose of seizing it immediately upon detecting the end of the previous transmission. Instead, it waits a random period of time
and then repeats the algorithm. Consequently, this algorithm leads to fewer collisions and better channel utilization but longer
delays than 1-persistent CSMA.

5) CSMA with Collision Detection (ETHERNET LAN)
Persistent and nonpersistent CSMA protocols are definitely an improvement over ALOHA because they ensure that no station begins
to transmit while the channel is busy. However, if two stations sense the channel to be idle and begin transmitting simultaneously,
their signals will still collide. Another improvement is for the stations to quickly detect the collision and abruptly stop
transmitting, (rather than finishing them) since they are irretrievably garbled anyway. This strategy saves time and bandwidth. 

This protocol, known as CSMA/CD (CSMA with Collision Detection), is the basis of the classic ETHERNET LAN, so it is worth devoting
some time to looking at it in detail. It is important to realize that collision detection is an analog process. The station’s
hardware must listen to the channel while it is transmitting. If the signal it reads back is different from the signal it is putting
out, it knows that a collision is occurring. The implications are that a received signal must not be tiny compared to the
transmitted signal (which is difficult for wireless, as received signals may be 1,000,000 times weaker than transmitted signals)
and that the modulation must be chosen to allow collisions to be detected.

--- COLLISION FREE PROTOCOLS
Although collisions do not occur with CSMA/CD once a station has unambiguously captured the channel, they can still occur during
the contention period. These collisions adversely affect the system performance, in particular when the bandwidth-delay product is
large, such as when the cable is long (i.e., large tao) and the frames are short. Not only do collisions reduce bandwidth, but they
make the time to send a frame variable, which is not a good fit for real-time traffic such as voice over IP. CSMA/CD is also not
universally applicable. In this section, we will examine some protocols that resolve the contention for the channel WITHOUT any
COLLISIONS at all, not even during the contention period. 

//skip

--- LIMITED CONTENTION PROTOCOLS
We have now considered two basic strategies for channel acquisition in a broadcast network: contention, as in CSMA, and
collision-free protocols. Each strategy can be rated as to how well it does with respect to the two important performance measures,
delay at low load and channel efficiency at high load. Under conditions of light load, contention is preferable due to its low delay
(since collisions are rare). As the load increases, contention becomes increasingly less attractive because the overhead associated
with channel arbitration becomes greater. Just the reverse is true for the collision-free protocols. At low load, they have
relatively high delay but as the load increases, the channel efficiency improves (since the overheads are fixed). Obviously, it
would be nice if we could combine the best properties of the contention and collision-free protocols, arriving at a new protocol
that used contention at low load to provide low delay, but used a collision-free technique at high load to provide good channel
efficiency. Such protocols, which we will call limited-contention protocols, do in fact exist, and will conclude our study of
carrier sense networks. 

//skip

--- WIRELESS LAN PROTOCOLS
We have already remarked that wireless systems cannot normally detect a collision while it is occurring. The received signal at a
station may be tiny, perhaps a million times fainter than the signal that is being transmitted. Finding it is like looking for a
ripple on the ocean. Instead, ACKNOWLEDGEMENTS are used to discover collisions and other errors after the fact. 

There is an even more important difference between wireless LANs and wired LANs. A station on a wireless LAN may not be able to
transmit frames to or receive frames from all other stations because of the limited radio range of the stations. In wired LANs,
when one station sends a frame, all other stations receive it. The absence of this property in wireless LANs causes a variety of
complications.

A naive approach to using a wireless LAN might be to try CSMA: just listen for other transmissions and only transmit if no one else
is doing so. The trouble is, this protocol is not really a good way to think about wireless because what matters for reception is
interference at the receiver, not at the sender. To see the nature of the problem, consider Fig. 4-11, where four wireless stations 
are illustrated. The radio range is such that A and B are within each other’s range and can potentially interfere with one another.
C can also potentially interfere with both B and D, but not with A. First consider what happens when A and C transmit to B. If A
sends and then C immediately senses the medium, it will not hear Abecause A is out of its range. Thus C will falsely conclude that 
it can transmit to B. If C does start transmitting, it will interfere at B, wiping out the frame from A.We want a MAC protocol that
will prevent this kind of collision from happening because it wastes bandwidth. The problem of a station not being able to detect
a potential competitor for the medium because the competitor is too far away is called the HIDDEN TERMINAL PROBLEM.

Now let us look at a different situation: B transmitting to A at the same time that C wants to transmit to D. If C senses the
medium, it will hear a transmission and falsely conclude that it may not send to D. In fact, such a transmission would cause bad
reception only in the zone between B and C, where neither of the intended receivers is located. We want a MAC protocol that prevents
this kind of deferral from happening because it wastes bandwidth. The problem is called the EXPOSED TERMINAL problem.

The difficulty is that, before starting a transmission, a station really wants to know whether there is radio activity around the
RECEIVER. CSMA merely tells it whether there is activity near the TRANSMITTER by sensing the carrier. With a wire, all signals 
propagate to all stations, so this distinction does not exist. However, only one transmission can then take place at once anywhere
in the system. In a system based on short-range radio waves, MULTIPLE transmissions can occur simultaneously if they all have
different destinations and these destinations are out of range of one another. We want this concurrency to happen as the cell gets
larger and larger, in the same way that people at a party should not wait for everyone in the room to go silent before they talk;
multiple conversations can take place at once in a large room as long as they are not directed to the same location.

An early and quite influential protocol that tackles these problems for wireless LANs is MACA (Multiple Access with Collision
Avoidance). The basic idea behind it is for the sender to stimulate the receiver into outputting a short frame, so stations nearby
can detect this transmission and avoid transmitting for the duration of the upcoming (large) data frame. This technique is used
instead of carrier sense.

Let us see how A sends a frame to B. A starts by sending an RTS (Request To Send) frame to B. This short frame (30 bytes) contains
the length of the data frame that will eventually follow. Then B replies with a CTS (Clear To Send ) frame. The CTS frame contains 
the data length (copied from the RTS frame). Upon receipt of the CTS frame, A begins transmission. Any station hearing the RTS is
clearly close to A and must remain silent long enough for the CTS to be transmitted back to A without conflict. Any station hearing
the CTS is clearly close to B and must remain silent during the upcoming data transmission, whose length it can tell by examining
the CTS frame.

Despite these precautions, collisions can still occur. For example, B and C could both send RTS frames to A at the same time. These
will collide and be lost. In the event of a collision, an unsuccessful transmitter (i.e., one that does not hear a CTS within the
expected time interval) waits a random amount of time and tries again later.